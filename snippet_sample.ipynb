{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5c5c29",
   "metadata": {},
   "source": [
    "Start with some demo cells for showing how to do markup and then change code (for those only registered in one workshop). Use the python example to explain python libraries. Then we will get our import statements out of the way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some standard libraries\n",
    "import os,re,sys\n",
    "from datetime import datetime\n",
    "\n",
    "# libraries for data mining and text analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import plotly.express as px\n",
    "\n",
    "# whoosh libraries - for creating and searching indexes\n",
    "from whoosh import scoring\n",
    "from whoosh.fields import Schema, DATETIME, ID, TEXT \n",
    "import whoosh.highlight as highlight\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "print(\"=> libraries loaded and ready to go...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18832dfd",
   "metadata": {},
   "source": [
    "nltk will download some building blocks, this could take a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the stopwords for several languages & VADER lexicon\n",
    "# some jupyter environments need custom paths\n",
    "#ntlk_path = os.sep + \"util\" + os.sep + \"odw\" + os.sep + \"nltk_data\"\n",
    "#nltk.data.path.append(ntlk_path)\n",
    "# alternative is nltk.download('stopwords')\n",
    "#nltk.download('stopwords',download_dir=ntlk_path)\n",
    "nltk.download('stopwords')\n",
    "# alternative is nltk.download('vader_lexicon')\n",
    "#nltk.download('vader_lexicon',download_dir=ntlk_path)\n",
    "nltk.download('vader_lexicon')\n",
    "print(\"=> downloading complete...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ece707",
   "metadata": {},
   "source": [
    "now some config options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3625e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These two values align the newspaper title and OCR directory (leave these alone for workshop since we are using one title)\n",
    "\"\"\"\n",
    "news_title = \"The Amherstburg Echo\" # newspaper title for indexing\n",
    "news_code = \"echo\" # used for location of OCR text\n",
    "print(\"=> newspaper title and OCR directory set...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83333724",
   "metadata": {},
   "source": [
    "Values needed for reative frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ecf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These values (on the other hand) are meant to be tinkered with.\n",
    "\"\"\"\n",
    "\n",
    "news_topics = [\"influenza\",\"flu\"]\n",
    "#news_topics = [\"pepsi\"]\n",
    "news_range = \"[1917 to 1919]\" # we use whoosh layout for date range\n",
    "print(\"=> relative frequency configuration set...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b619bae",
   "metadata": {},
   "source": [
    "With kudos to [jcoliver's](https://github.com/jcoliver) [Collections as Data](https://github.com/OurDigitalWorld/data_samples/blob/5651ca21a5a3b5186a2cbabdaed844dae1486818//repository) site. This is a slight variation on one of the exercises in the [Introduction to text mining](https://github.com/jcoliver/dig-coll-borderlands/blob/main/Text-Mining-Short.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c73cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = []\n",
    "\n",
    "range = re.findall(r'\\d+', news_range)\n",
    "folder_list = sorted(os.listdir(news_code))\n",
    "folder_paths = [os.path.join(news_code,i) for i in folder_list]\n",
    "for folder_path in folder_paths:\n",
    "    folder = folder_path.split(os.sep)[1]\n",
    "    if int(range[0]) <= int(folder) <= int(range[1]):\n",
    "        file_list = sorted(os.listdir(folder_path))\n",
    "        file_paths = [os.path.join(folder_path,j) for j in file_list]\n",
    "        for file in file_paths:\n",
    "            fp = open(file,'r', encoding='utf8')\n",
    "            text = fp.read()\n",
    "            text = \" \".join(text.split())\n",
    "            fp.close()\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            word_list = tokenizer.tokenize(text.lower())\n",
    "            word_table = pd.Series(word_list,dtype='string')\n",
    "            # Calculate relative frequencies of all words in the issue\n",
    "            word_freqs = word_table.value_counts(normalize = True)\n",
    "            # Pull out only values that match words of interest\n",
    "            my_freqs = word_freqs.filter(news_topics)\n",
    "            # Get the total frequency for words of interest\n",
    "            total_my_freq = my_freqs.sum()\n",
    "            skip = len(news_code) + 6\n",
    "            dates.append([file[skip:skip + 10],total_my_freq])\n",
    "            \n",
    "# add those dates to a data frame\n",
    "results_table = pd.DataFrame(dates, columns = ['Date','Frequency']) \n",
    "# Analyses are all done, plot the figure\n",
    "my_figure = px.line(results_table, x = 'Date', y = 'Frequency').update_layout(yaxis_title=\"Relative Freq.\")\n",
    "print(\"=> pages examined:\", len(dates))\n",
    "my_figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedfb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Configuration options for Sentiment Analysis \"\"\"\n",
    "\n",
    "index_dir = \"echo_whoosh\" # directory for index\n",
    "#news_query = \"wom?n OR female*\" # follow whoosh conventions, e.g \"wom?n OR female*\"\n",
    "news_query = \"road*\"\n",
    "index_range = \"[19170601 to 19190601]\" # follow whoosh conventions, e.g \"1975\", \"[1970 to 1980]\", \"[19000101 to 19000431]\"\n",
    "snippet_limit = 200 # limit for number of snippets to work with\n",
    "print(\"=> configuration set...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab9fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classes and functions are here in one place.\n",
    "\"\"\"\n",
    "\n",
    "class MinimalFormatter(highlight.Formatter):\n",
    "\n",
    "    def format_token(self, text, token, replace=False):\n",
    "        tokentext = highlight.get_text(text, token, replace)\n",
    "\n",
    "        # this could be elaborate as shown \n",
    "        # return \"[%s]\" % tokentext\n",
    "\n",
    "        # but just return the token here\n",
    "        return tokentext\n",
    "\n",
    "def createSearchableData(root,indexdir):   \n",
    " \n",
    "    # Note that we need content to be stored for highlighting to work\n",
    "    schema = Schema(title=TEXT(stored=True),\n",
    "              path=ID(stored=True),\n",
    "              content=TEXT(stored=True),\n",
    "              pubdate=DATETIME(stored=True))\n",
    "\n",
    "    # this is how a whoosh index can be created\n",
    "    # ideally, this would be done outside of the notebook\n",
    "    # for a large set\n",
    "    if not os.path.exists(indexdir):\n",
    "        os.mkdir(indexdir)\n",
    " \n",
    "        # Creating an index writer to add documents\n",
    "        ix = create_in(indexdir,schema)\n",
    "        writer = ix.writer()\n",
    " \n",
    "        # Assume file text is local\n",
    "        folder_list = sorted(os.listdir(root))\n",
    "        folder_paths = [os.path.join(root,i) for i in folder_list]\n",
    "        for folder_path in folder_paths:\n",
    "            print(folder_path)\n",
    "            file_list = sorted(os.listdir(folder_path))\n",
    "            file_paths = [os.path.join(folder_path,j) for j in file_list]\n",
    "            for file_path in file_paths:\n",
    "                fp = open(file_path,'r', encoding='utf8')\n",
    "                file_bits = file_path.split(os.sep)\n",
    "                page_id = file_bits[len(file_bits) - 1]\n",
    "                page_id = page_id.replace(\".txt\",\"\")\n",
    "                date_str = page_id[:10]\n",
    "                date_object = datetime.strptime(date_str,\"%Y-%m-%d\")\n",
    "                page_num = int(page_id[11:])\n",
    "                ntitle = date_object.strftime(\"%B %d, %Y\") + \"- pg. \" + str(page_num)\n",
    "                text = fp.read()\n",
    "                text = \" \".join(text.split())\n",
    "                writer.add_document(title = news_title + \". \" + ntitle,\n",
    "                    path=page_id, content=text, pubdate = date_object)\n",
    "                fp.close()\n",
    "                \n",
    "        print(\"commiting...\") # this can be the slowest step\n",
    "        writer.commit() \n",
    "    if os.path.exists(indexdir):\n",
    "        print(\"=> index directory exists...\")\n",
    "        \n",
    "print(\"=> classes & functions in place...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fa5712",
   "metadata": {},
   "source": [
    "An index only has to be created once (if the data has not changed). This next cell can be skipped if the index is already there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "createSearchableData(news_code,index_dir) # this will index the OCR text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163e1a3",
   "metadata": {},
   "source": [
    "Now use the index for getting highlights. The highlights will be our snippets. Whoosh has the plumbing for something far more elaborate but keep it simple for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02adf32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the index directory contains the index\n",
    "ix = open_dir(index_dir)\n",
    " \n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(news_query)\n",
    "allow_q = qp.parse(\"pubdate:\" + index_range)\n",
    "#allow_q = qp.parse(u\"pubdate:19510104\")\n",
    "#allow_q = qp.parse(u\"pubdate:[19700101 to 19801231]\")\n",
    "#q = qp.parse(u\"19500104\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q,filter=allow_q,limit=snippet_limit) \n",
    "    # Allow larger fragments\n",
    "    results.fragmenter.maxchars = 50\n",
    "\n",
    "    # Show more context before and after\n",
    "    results.fragmenter.surround = 5\n",
    "\n",
    "    # using the class above\n",
    "    minf = MinimalFormatter()\n",
    "    results.formatter = minf\n",
    "\n",
    "    snippets = []\n",
    "    i = 0\n",
    "    for i,hit in enumerate(results):\n",
    "        # clean up the spaces in the result\n",
    "        snippet = \" \".join(hit.highlights(\"content\").split())\n",
    "        snippets.append([snippet,hit[\"path\"][:4],hit[\"path\"]])\n",
    "print(\"=> # of snippets gathered: \", 0 if i == 0 else i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95490bf1",
   "metadata": {},
   "source": [
    "At this point, the snippets/highlights are collected. Now we handover the results to the powerful [pandas](https://pandas.pydata.org/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4667b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(snippets,columns=['snippet','year','page'])\n",
    "df[\"row_id\"] = df.index + 1\n",
    "\n",
    "#remove all non-alphabet characters\n",
    "df['snippet'] = df['snippet'].str.replace(\"[^a-zA-Z#]\", \" \", regex=True)\n",
    "#covert to lower-case\n",
    "df['snippet'] = df['snippet'].str.casefold()\n",
    "\n",
    "print(\"=> the handover from whoosh to pandas is complete...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff9619",
   "metadata": {},
   "source": [
    "The following is based on the redgate tutorial at [Sentiment Analysis with Python](https://www.red-gate.com/simple-talk/development/data-science-development/sentiment-analysis-python/). This is for illustrative purposes, [VADER](https://github.com/cjhutto/vaderSentiment) may not be the right tool for historical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "tmp = [['99999999999','NA999NA',0]]\n",
    "for index, row in df.iterrows():\n",
    "    scores = sid.polarity_scores(row[0])   \n",
    "    for key, value in scores.items():\n",
    "        #row is is the last column in tmp\n",
    "        tmp.append([row[3],key,value])\n",
    "\n",
    "# this is a slight variation, the original append method is being depreciated\n",
    "t_df=pd.DataFrame(tmp,columns=['row_id','sentiment_type','sentiment_score'])\n",
    "\n",
    "# remove dummy row with row_id = 99999999999\n",
    "t_df_cleaned = t_df[t_df.row_id != '99999999999']\n",
    "# remove duplicates if any exist\n",
    "t_df_cleaned = t_df_cleaned.drop_duplicates()\n",
    "# only keep rows where sentiment_type = compound\n",
    "t_df_cleaned = t_df[t_df.sentiment_type == 'compound']\n",
    "# merge dataframes\n",
    "df_output = pd.merge(df, t_df_cleaned, on='row_id', how='inner')\n",
    "\n",
    "# take a look at first few entries for negative scores\n",
    "df_belowzero = df_output[df_output.sentiment_score < 0.0]\n",
    "print(\"len negative\",len(df_belowzero))\n",
    "print(df_belowzero.head())\n",
    "#print(\"total\",len(df_output))\n",
    "\n",
    "df_abovezero = df_output[df_output.sentiment_score > 0.0]\n",
    "print(\"len positive\",len(df_abovezero))\n",
    "print(\"total\",len(df_output))\n",
    "print(df_abovezero.head())\n",
    "\n",
    "df_zero = df_output[df_output.sentiment_score == 0.0]\n",
    "print(\"len zero\",len(df_zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f97c8",
   "metadata": {},
   "source": [
    "See above link for explaination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f60006",
   "metadata": {},
   "source": [
    "Get some details on the scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28943146",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output[[\"sentiment_score\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b4687",
   "metadata": {},
   "source": [
    "The graphing is a little misdirected in my sample, it would make more sense to compare time periods around an event, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate mean of sentiment_score by snippet\n",
    "dfg = df_output.groupby(['year'])['sentiment_score'].mean()\n",
    "# create a bar plot\n",
    "\n",
    "dfg.plot(kind='bar', title='Sentiment Score', ylabel='Mean Sentiment Score',\n",
    "         xlabel='Year', figsize=(6, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880b61b",
   "metadata": {},
   "source": [
    "And that's it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e1803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
