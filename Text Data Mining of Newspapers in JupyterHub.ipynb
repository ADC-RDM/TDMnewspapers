{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c5a5ab",
   "metadata": {},
   "source": [
    "# Part 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c5c29",
   "metadata": {},
   "source": [
    "This workshop is part of a [series](https://leddy.uwindsor.ca/rdm-tdm-jupyterhub-newspapers) on _Research Data Management_ and _Text Data Mining_. This particular session attempts to bring together the use of textual analysis with digitized newspaper content within a [JupyterHub](https://jupyter.org/hub) environment. The session does not require background in any of these areas, but for those who may be attending this session without the benefit of the preceeding _Introduction to JupyterHub_ workshop, it is useful to understand some conventions of [Jupyter](https://jupyter.org/) notebooks.\n",
    "\n",
    "JupyterHub is a collaborative environment for the use of Jupyter notebooks, and thanks to the work of the [Digital Alliance](https://alliancecan.ca/) and others, including [Sharcnet/Compute Ontario](https://www.sharcnet.ca/), JupyterHub is increasingly becoming an entry point to research computing in Canada. The notebooks made available in JupyterHub are digital documents, analogous to the physical notebooks used for research in labs and studies. A Jupyter notebook consists of \"cells\", which typically contain descriptive text (like what you are reading right now), or code. Today's session will use [python](https://www.python.org/doc/essays/blurb/) coding statements, but Jupyter notebooks can use other programming languages as well. There is information at the end of this notebook if you are interested in learning more about Jupyter, but we will start with two simple exercises to illustrate how Jupyter works.\n",
    "\n",
    "To start with, you will be asked double-click on the next paragraph (but finish reading this one first). Your challenge will be to put an underscore before and after the word _Jupyter_, like this: \\_Jupyter\\_. After you have done so, select the \"Run\" button at the top of the screen, or use Ctrl-Enter (the keyboard shortcuts may not always be completely consistent across Jupyter environments, but the \"Run\" button should always work). Go ahead, give it a try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f881975b",
   "metadata": {},
   "source": [
    "Hi there, my name is Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ca5bcd",
   "metadata": {},
   "source": [
    "You probably noticed that the paragraph was contained within what looks like a box. That is the Jupyter \"cell\", it is a way to organized text and code. The underscore characters you added are examples of [Markdown](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html), sort of a simplified HTML scheme, that allows some formatting options without requiring a lot of extra steps. There are many tools for Markdown, and options to help with more elaborate text challenges, like tables, but the main goal of Markdown is to be simple and not get in the way of explaining and describing what the code provided is trying to achieve.\n",
    "\n",
    "And that brings us to modifying a cell with actual code. Double click the next cell and change the word \"World\" to your name, or something else. Use the \"Run\" button again and you will hopefully see a change in the text below the cell. You are actually running code in this exercise, your notebook is empowered to let you alter and execute program statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World! My name is Jupyter.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327f025d",
   "metadata": {},
   "source": [
    "Congratulations! By editing the two cells in the above exercises, you are most of the way there in making use of Jupyter Notebooks. Jupyter brings together text and code to provide an environment for explanatory documents. You don't necessarily have to understand how Jupyter notebooks work to benefit from them, but hopefully this short introduction gives you a sense of how  notebooks work.\n",
    "\n",
    "\n",
    "### Key takeaways:\n",
    "* Jupyter notebooks are interactive.\n",
    "* Jupyter notebooks support explanations.\n",
    "* JupyterHub can be your gateway to resources that are otherwise unavailable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6409e",
   "metadata": {},
   "source": [
    "# Part 2: Using newspapers as a source for text data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9273a3",
   "metadata": {},
   "source": [
    "\n",
    "This workshop will be using an historical newspaper from Essex County, Ontario called [_The Amherstburg Echo_](http://ink.scholarsportal.info/echo). The _Echo_ was purposely selected because it both illustrates many of the challenges associated with newspaper digitization, especially for community newspapers, but also because it was a consistently high quality title from its beginnings in 1874 until it was closed in 2012. \n",
    "\n",
    "Digitization probably pales in comparison to the challenges of keeping newspapers afloat financially, and Ontario has seen many newspapers shut down in recent years, but long before the internet, newspapers were a key source of information for communities. Historical newspaper digitization is a huge topic and is mostly out-of-scope for the workshop, but here are a [few slides](https://docs.google.com/presentation/d/1xHqzNIjqIV8rMLRbysLI8lK07H352fSEbkQOPVTSd0E/edit?usp=sharing) to help put newspapers in context for text processing.\n",
    "\n",
    "In addition to trying to understand the source of the text, it is also worth flagging the sheer amount of text that historical newspapers can represent and some of the difficulties introduced by the use of [Optical Character Recogniton](https://en.wikipedia.org/wiki/Optical_character_recognition) (OCR). Here are some metrics in spreadsheets from the [New York Times](https://docs.google.com/spreadsheets/d/1GQM_7qszC2VBbsPSqDPr7rJXHZ9rmGz4slIO9dWAL8Q/edit#gid=0) (courtesy of the [Internet Archive](https://archive.org/)) and a comparable example from the [Echo](https://docs.google.com/spreadsheets/d/1rCGnQF5cNqSRWS3QxQmBSWRaGdSmC4zOyA_QqjbPVJM/edit#gid=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac7537",
   "metadata": {},
   "source": [
    "# Part 3: Text Mining up close - Spanish Flu example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d8e673",
   "metadata": {},
   "source": [
    "For this example, we will build on the great work done by the University of Arizona's [Newspapers as Data](https://libguides.library.arizona.edu/newspapers-as-data) project. In order to make sure all of the building blocks we will need are in place in our environment, we will gather all of our _import_ statements together in one cell. In python, the _import_ statement is used to specify program libraries that will be called on for specific tasks. By importing the libraries alll at once, we will know immediately whether we have everything we need to run other cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some standard libraries\n",
    "import os,re,sys\n",
    "from datetime import datetime\n",
    "\n",
    "# libraries for data mining and text analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import plotly.express as px\n",
    "\n",
    "# whoosh libraries - for creating and searching indexes\n",
    "from whoosh import scoring\n",
    "from whoosh.fields import Schema, DATETIME, ID, TEXT \n",
    "import whoosh.highlight as highlight\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "print(\"=> libraries loaded and ready to go...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18832dfd",
   "metadata": {},
   "source": [
    "It is not necessary to understand all of the python syntax, but notice that the cell above ends with a print statement. This gives some additional indicaton that the process has been completed, which is sometimes easy to miss in a notebook. One python library that we will use immediately is the [Natural Language Toolkit](https://www.nltk.org/) or _NLTK_. The NLTK has support for downloading data that is useful for text processing, and like the python libraries above, it is worth establishing at the outset that the data can be accessed and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download the stopwords for several languages & VADER lexicon.\n",
    "Note that some jupyter environments need custom paths, which can be\n",
    "achieved with statements like:\n",
    "   ntlk_path = os.sep + \"util\" + os.sep + \"odw\" + os.sep + \"nltk_data\"\n",
    "   nltk.data.path.append(ntlk_path)\n",
    "   nltk.download('stopwords',download_dir=ntlk_path)\n",
    "\"\"\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "print(\"=> downloading complete...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ece707",
   "metadata": {},
   "source": [
    "At this point, we can start providing configuration values for the newspaper we are working with. Here are some values which will not be changed again in the workshop, but could be altered for another newspaper title, or customized for a different project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3625e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These values align the newspaper title and OCR directory (leave these alone for workshop since we are using one title)\n",
    "\"\"\"\n",
    "\n",
    "news_title = \"The Amherstburg Echo\" # newspaper title for indexing\n",
    "news_code = \"echo\" # used for location of OCR text\n",
    "url_base = \"https://collections.uwindsor.ca/workshop/\" + news_code # url for viewing PDFs\n",
    "print(\"=> newspaper title and OCR directory set...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83333724",
   "metadata": {},
   "source": [
    "We will try a slight variation on an exercise in the University of Arizona's [Introduction to text mining](https://github.com/jcoliver/dig-coll-borderlands/blob/main/Text-Mining-Short.ipynb) notebook. The basic idea is to calculate the relative frequency of terms associated with the [Spanish Flu](https://www.theglobeandmail.com/canada/article-mandatory-masks-shuttered-theatres-and-confusing-rules-the-1918/) for the years 1917 to 1919, which was the height of this afflication in Ontario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ecf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These values define what terms will be and what years will be examined.\n",
    "\"\"\"\n",
    "\n",
    "news_topics = [\"influenza\",\"flu\"]\n",
    "news_range = \"[1917 to 1919]\" # we use whoosh layout for date range\n",
    "print(\"=> relative frequency configuration set...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179e818",
   "metadata": {},
   "source": [
    "Now some python coding to go through the full text of the newspapers for the years specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c73cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The folders containing OCR text are organized by year. For the years identified\n",
    "above, calculate relative frequencies (specified term compared to other terms),\n",
    "and then graph the results.\n",
    "\"\"\"\n",
    "dates = [] # dates and frequencies will be collection here\n",
    "\n",
    "range = re.findall(r'\\d+', news_range)\n",
    "folder_list = sorted(os.listdir(news_code))\n",
    "folder_paths = [os.path.join(news_code,i) for i in folder_list]\n",
    "for folder_path in folder_paths:\n",
    "    folder = folder_path.split(os.sep)[1]\n",
    "    if int(range[0]) <= int(folder) <= int(range[1]):\n",
    "        file_list = sorted(os.listdir(folder_path))\n",
    "        file_paths = [os.path.join(folder_path,j) for j in file_list]\n",
    "        for file in file_paths:\n",
    "            if file.endswith('.txt'):\n",
    "                fp = open(file,'r', encoding='utf8')\n",
    "                text = fp.read()\n",
    "                text = \" \".join(text.split())\n",
    "                fp.close()\n",
    "                tokenizer = RegexpTokenizer(r'\\w+')\n",
    "                word_list = tokenizer.tokenize(text.lower())\n",
    "                word_table = pd.Series(word_list,dtype='string')\n",
    "                # Calculate relative frequencies of all words in the issue\n",
    "                word_freqs = word_table.value_counts(normalize = True)\n",
    "                # Pull out only values that match words of interest\n",
    "                my_freqs = word_freqs.filter(news_topics)\n",
    "                # Get the total frequency for words of interest\n",
    "                total_my_freq = my_freqs.sum()\n",
    "                # The file names are used to identify dates\n",
    "                skip = len(news_code) + 6\n",
    "                dates.append([file[skip:skip + 10],total_my_freq])\n",
    "            \n",
    "# Add those dates to a data frame\n",
    "results_table = pd.DataFrame(dates, columns = ['Date','Frequency']) \n",
    "# Analyses are all done, plot the figure\n",
    "my_figure = px.line(results_table, x = 'Date', y = 'Frequency').update_layout(yaxis_title=\"Relative Freq.\")\n",
    "print(\"=> pages examined:\", len(dates))\n",
    "# Show figure\n",
    "my_figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c043e2a",
   "metadata": {},
   "source": [
    "# Part 4: Sentiment Analysis of historical newspapers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ceeb4",
   "metadata": {},
   "source": [
    "[Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) typically involves the calculation of polarity scores is a method which will give us numbers for the following categories:\n",
    "\n",
    "* Positive\n",
    "* Negative\n",
    "* Neutral\n",
    "* Compound\n",
    "\n",
    "The compound score is the sum of positive, negative snf neutral scores which is then normalized between -1 (most extreme negative) and +1 (most extreme positive). The calculation is often based on a lexicon (dictionary of terms with weight assignments), a popular lexicon is [VADER](https://github.com/cjhutto/vaderSentiment) (Valence Aware Dictionary and sEntiment Reasoner), a general purpose system. VADER  has support for social media, but has been used for [historical text](https://programminghistorian.org/en/lessons/sentiment-analysis) as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7816aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Examine polarity scores for sample sentences.\n",
    "\"\"\"\n",
    "\n",
    "sample_sentence = \"My cat is a good cat.\"\n",
    "sentiment = SentimentIntensityAnalyzer() # this analyzer is specific VADER\n",
    "print(sentiment.polarity_scores(sample_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2b87c",
   "metadata": {},
   "source": [
    "Once again, we will put our configuration options together in one cell. The approach here is to use an _index_ of the OCRed text of the newspaper to collect hightlights or snippets of text surrounding term(s) of interest. The indexing is carried out by a python searching system called [Whoosh](https://whoosh.readthedocs.io/), which is very similar to the popular [Lucene](https://lucene.apache.org/) search engine. Like Lucene, Whoosh can retrieve snippets surrounding search terms. The idea is that the snippet will provide enough sentence structure to hopefully calculate a meaningful compound polarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedfb90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Configuration options for Whoosh searching \n",
    "\"\"\"\n",
    "\n",
    "index_dir = \"whoosh_index\" # directory for index\n",
    "snippet_limit = 200 # limit for number of snippets to work with\n",
    "print(\"=> configuration set...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f97c13",
   "metadata": {},
   "source": [
    "The next cell contains the python logic to build the Whoosh index and a few related functions. Note that the index could be built outside of the notebook and probably should be for large newspaper sets. Github has some limits on file size that makes providing an index problematic, which can be a limitation that carries over to _Binder_, and it is possible that building the index at the time is a better approach. On the other hand, some _JupyterHub_ environments have ample space for providing prebuild data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab9fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classes and functions are here in one place.\n",
    "\"\"\"\n",
    "\n",
    "class MinimalFormatter(highlight.Formatter):\n",
    "\n",
    "    def format_token(self, text, token, replace=False):\n",
    "        tokentext = highlight.get_text(text, token, replace)\n",
    "\n",
    "        # this could be elaborate as shown \n",
    "        # return \"[%s]\" % tokentext\n",
    "\n",
    "        # but just return the token here\n",
    "        return tokentext\n",
    "\n",
    "def createUrl(np_page):\n",
    "    # utility functon for later\n",
    "    year = np_page[:4]\n",
    "    month = np_page[5:7]\n",
    "    day = np_page[8:10]\n",
    "    seq = np_page[11:]\n",
    "    print(url_base + \"/\" + year + \"/\" + year + \"_\" + month + \"/\" + np_page + \".pdf\")\n",
    "    \n",
    "def createSearchableData(root,indexdir):   \n",
    " \n",
    "    # Note that we need content to be stored for highlighting to work\n",
    "    schema = Schema(title=TEXT(stored=True),\n",
    "              path=ID(stored=True),\n",
    "              content=TEXT(stored=True),\n",
    "              pubdate=DATETIME(stored=True))\n",
    "\n",
    "    # this is how a whoosh index can be created\n",
    "    # ideally, this would be done outside of the notebook\n",
    "    # for a large set\n",
    "    if not os.path.exists(indexdir):\n",
    "        os.mkdir(indexdir)\n",
    " \n",
    "        # Creating an index writer to add documents\n",
    "        ix = create_in(indexdir,schema)\n",
    "        writer = ix.writer()\n",
    " \n",
    "        # Assume file text is local\n",
    "        folder_list = sorted(os.listdir(root))\n",
    "        folder_paths = [os.path.join(root,i) for i in folder_list]\n",
    "        for folder_path in folder_paths:\n",
    "            print(folder_path)\n",
    "            file_list = sorted(os.listdir(folder_path))\n",
    "            file_paths = [os.path.join(folder_path,j) for j in file_list]\n",
    "            for file_path in file_paths:\n",
    "                if file_path.endswith('.txt'):\n",
    "                    fp = open(file_path,'r', encoding='utf8')\n",
    "                    file_bits = file_path.split(os.sep)\n",
    "                    page_id = file_bits[len(file_bits) - 1]\n",
    "                    page_id = page_id.replace(\".txt\",\"\")\n",
    "                    date_str = page_id[:10]\n",
    "                    date_object = datetime.strptime(date_str,\"%Y-%m-%d\")\n",
    "                    page_num = int(page_id[11:])\n",
    "                    ntitle = date_object.strftime(\"%B %d, %Y\") + \"- pg. \" + str(page_num)\n",
    "                    text = fp.read()\n",
    "                    text = \" \".join(text.split())\n",
    "                    writer.add_document(title = news_title + \". \" + ntitle,\n",
    "                        path=page_id, content=text, pubdate = date_object)\n",
    "                    fp.close()\n",
    "                \n",
    "        print(\"commiting...\") # this can be the slowest step\n",
    "        writer.commit() \n",
    "    if os.path.exists(indexdir):\n",
    "        print(\"=> index directory exists...\")\n",
    "        \n",
    "print(\"=> classes & functions in place...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fa5712",
   "metadata": {},
   "source": [
    "An index only has to be created once (if the data has not changed). This next cell can be skipped if the index is already there. The GitHub set of newspaper text will take a few minutes to be indexed by Whoosh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is where we build the index. This could take some time,\n",
    "depending on the JupyterHub environment.\n",
    "\"\"\"\n",
    "\n",
    "createSearchableData(news_code,index_dir) # a great time to look at folder structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b163e1a3",
   "metadata": {},
   "source": [
    "Now use the index for getting highlights/snippets. Whoosh has the plumbing for more elaborate searching but keep it simple for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02adf32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Whoosh steps in - this is where to search the index\n",
    "\"\"\"\n",
    "\n",
    "# follow whoosh conventions for terms, very similar to lucene, e.g \"wom?n OR female*\"\n",
    "news_query = \"smok*\"\n",
    "index_range = \"[1917 to 1918]\" # follow whoosh conventions for dates, e.g \"1975\", \"[1970 to 1980]\", \"[19000101 to 19000431]\"\n",
    "\n",
    "# the index directory contains the index\n",
    "ix = open_dir(index_dir)\n",
    " \n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(news_query)\n",
    "allow_q = qp.parse(\"pubdate:\" + index_range)\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q,filter=allow_q,limit=snippet_limit) \n",
    "    # Allow larger fragments\n",
    "    results.fragmenter.maxchars = 100\n",
    "\n",
    "    # Show more context before and after\n",
    "    results.fragmenter.surround = 10\n",
    "\n",
    "    # Use the class defined above to strip HTML tags around terms\n",
    "    minf = MinimalFormatter()\n",
    "    results.formatter = minf\n",
    "\n",
    "    snippets = []\n",
    "    i = 0\n",
    "    for i,hit in enumerate(results):\n",
    "        # clean up the spaces in the result\n",
    "        snippet = \" \".join(hit.highlights(\"content\").split())\n",
    "        snippets.append([snippet,hit[\"path\"][:4],hit[\"path\"]])\n",
    "print(\"=> # of snippets gathered: \", 0 if i == 0 else i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95490bf1",
   "metadata": {},
   "source": [
    "At this point, the snippets/highlights are collected. Now we handover the results to the powerful [Pandas](https://pandas.pydata.org/) library. Pandas has a data structure called a _DataFrame_, sort of like an internal spreadsheet or database table, which is extremely valuable for data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Take a look at a few snippets\n",
    "\"\"\"\n",
    "\n",
    "for snippet in snippets[:10]:\n",
    "    print(\"snippet\", snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e61d5bf",
   "metadata": {},
   "source": [
    "# Part 5: From searching to data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b37d240",
   "metadata": {},
   "source": [
    "The text from the index is not quite ready for data processing. The following cell will convert the results to a format that can be handled by the Sentiment Analysis tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4667b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The Whoosh results are set up for a DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "df = pd.DataFrame(snippets,columns=['snippet','year','page'])\n",
    "df[\"row_id\"] = df.index + 1\n",
    "\n",
    "# remove all non-alphabet characters\n",
    "df['snippet'] = df['snippet'].str.replace(\"[^a-zA-Z#]\", \" \", regex=True)\n",
    "# covert to lower-case\n",
    "df['snippet'] = df['snippet'].str.casefold()\n",
    "\n",
    "print(\"=> the handover from whoosh to pandas is complete...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff9619",
   "metadata": {},
   "source": [
    "The following is based on the redgate tutorial at [Sentiment Analysis with Python](https://www.red-gate.com/simple-talk/development/data-science-development/sentiment-analysis-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is where the polarity scores are calculated.\n",
    "\"\"\"\n",
    "\n",
    "tmp = []\n",
    "df_output = None\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for index, row in df.iterrows():\n",
    "    scores = sid.polarity_scores(row[0])   \n",
    "    for key, value in scores.items():\n",
    "        #row is is the last column\n",
    "        tmp.append([row[3],key,value])\n",
    "\n",
    "if len(tmp) > 0:\n",
    "    # this is a slight variation, the original append method has been depreciated\n",
    "    t_df=pd.DataFrame(tmp,columns=['row_id','sentiment_type','sentiment_score'])\n",
    "    # remove duplicates if any exist\n",
    "    t_df_cleaned = t_df.drop_duplicates()\n",
    "    # only keep rows where sentiment_type = compound\n",
    "    t_df_cleaned = t_df_cleaned[t_df.sentiment_type == 'compound']\n",
    "    # merge dataframes - this unites the snippets with scores\n",
    "    df_output = pd.merge(df, t_df_cleaned, on='row_id', how='inner')\n",
    "\n",
    "print(\"=> we have a DataFrame with the scores...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c2ded",
   "metadata": {},
   "source": [
    "The DataFrame can now be examined with some nifty builtin functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19616871",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output[[\"sentiment_score\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f97c8",
   "metadata": {},
   "source": [
    "We can look more closely at some of the scores and the underlying snippets. This will help give a sense of how the numbers match the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use some inbuilt Pandas functions to look at scoring.\n",
    "\"\"\"\n",
    "\n",
    "# take a look at first few entries for negative scores\n",
    "df_belowzero = df_output[df_output.sentiment_score < 0.0]\n",
    "df_snippets = df_belowzero[[\"sentiment_score\",\"snippet\",\"page\"]]\n",
    "print(\"=== negative scores: %d out of %d ===\" % (len(df_snippets),len(df_output)))\n",
    "print(df_snippets.head(10))\n",
    "\n",
    "df_abovezero = df_output[df_output.sentiment_score > 0.0]\n",
    "df_snippets = df_abovezero[[\"sentiment_score\",\"snippet\",\"page\"]]\n",
    "print(\"=== positive scores: %d out of %d ===\" % (len(df_snippets),len(df_output)))\n",
    "print(df_snippets.head(10))\n",
    "\n",
    "df_zero = df_output[df_output.sentiment_score == 0.0]\n",
    "df_snippets = df_zero[[\"sentiment_score\",\"snippet\",\"page\"]]\n",
    "print(\"=== zero scores: %d out of %d ===\" % (len(df_snippets),len(df_output)))\n",
    "print(df_snippets.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c6064c",
   "metadata": {},
   "source": [
    "If we want to see the content associated with the snippet, we can use a convenience function to look at a PDF with embedded OCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74623e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copy and paste from above for snippets of interest.\n",
    "\"\"\"\n",
    "\n",
    "createUrl(\"1918-12-06-0006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f60006",
   "metadata": {},
   "source": [
    "Graphing the results by year is one approach to tracking sentiment over time. Dataframes make creating graphs a fairly simple process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Graph DataDrame by year\n",
    "\"\"\"\n",
    "\n",
    "# generate mean of sentiment_score by snippet\n",
    "dfg = df_output.groupby(['year'])['sentiment_score'].mean()\n",
    "# create a bar plot\n",
    "\n",
    "dfg.plot(kind='bar', title='Sentiment Score', ylabel='Mean Sentiment Score',\n",
    "         xlabel='Year', figsize=(6, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59cd2f3",
   "metadata": {},
   "source": [
    "# Part 6: Where to next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a36082",
   "metadata": {},
   "source": [
    "This workshop has hopefully showed a bit of what is possible with Jupyter and historical newspapers. Here are some resources for further exploration of what has been presented here.\n",
    "\n",
    "### Jupyter/JupyterHub\n",
    "\n",
    "* [Introduction to Jupyter Notebooks](https://programminghistorian.org/en/lessons/jupyter-notebooks) - from the [Programming Historian](https://programminghistorian.org/).\n",
    "\n",
    "* [JupyterHub](https://jupyterhub.readthedocs.io/) - the official docs for JupyterHub. \n",
    "\n",
    "* [Anaconda](https://www.anaconda.com/products/distribution) - an easy option for installing Jupyter across platforms.\n",
    "\n",
    "### Text Analysis/Data Processing\n",
    "\n",
    "* [Introduction to Pandas](https://towardsdatascience.com/introduction-to-pandas-hands-on-tutorial-part-one-2e74f35ab166) - a short and to the point tutorial that uses _Anaconda_.\n",
    "\n",
    "* [Introduction to Natural Language Processing using NLTK](https://blog.paperspace.com/introduction-to-natural-language-processing-using-nltk/) - a quick run-through of _NLTK_ functions.\n",
    "\n",
    "### Newspaper Digitization/Newspapers as Data\n",
    "\n",
    "* [Newspapers as Data](https://libguides.library.arizona.edu/newspapers-as-data) - the starting point for the University of Arizona's efforts to support student data literacy with newspapers.\n",
    "\n",
    "* [Collections As Data](https://collectionsasdata.github.io/part2whole/) - this Mellon-funded initiative provided support for Arizona's great work, this site included the grant justification, and information about the concluding international summit, [Collections as Data: State of the Field and Future Directions](https://collectionsasdata.github.io/part2whole/future/), to be held from April 25-26, 2023 at [Internet Archive Canada](https://internetarchivecanada.org/).\n",
    "\n",
    "* [Macrophotography from Microfilm](https://ourdigitalworld.net/2020/07/20/macrophotography-from-microfilm/) - no organization has digitized more newspapers with fewer resources than [OurDigitalWorld](https://ourdigitalworld.net/) (ODW), this is one of many initiatives ODW has supported to bring costs down in order to open the possibilities of newspaper digitization to organizations of all sizes."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
